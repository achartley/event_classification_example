{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using dense neural network (fully connected)\n",
    "For this exercise we'll be using [Keras](https://keras.io/).. In addition you need either [TensorFlow](https://www.tensorflow.org/) or [PyTorch](https://pytorch.org/)\n",
    "as the underlying framework. Or, you can also continue with scikit-learn, which also has dense / fully connected networks implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of image data\n",
    "We apply the same normalization as for logistic regression. Recall that we have saved this function in a file\n",
    "called `helper_functions.py`, and can import and use it directly from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score, \n",
    "                             matthews_corrcoef, roc_curve, roc_auc_score)\n",
    "from helper_functions import normalize_image_data, plot_roc_auc\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels.\n",
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "images = np.load(DATA_PATH+\"images_training.npy\")\n",
    "labels = np.load(DATA_PATH+\"labels_training.npy\")\n",
    "\n",
    "# Just like with logistic regression, we need to reshape the images to\n",
    "# be one-dimensional for the input to the model.\n",
    "images = images.reshape(images.shape[0], 256)\n",
    "\n",
    "# Split the training indices into training and validation. \n",
    "# Validate with 25% of the data (default). Can be adjusted.\n",
    "x_idx = np.arange(images.shape[0])\n",
    "train_idx, val_idx, not_used1, not_used2 = train_test_split(x_idx, x_idx, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model\n",
    "We're going to start off with a simple feed-forward neural network one hidden layer. This is a binary classifier, so\n",
    "we techincally only need it to output one number.\n",
    "\n",
    "What about the number of nodes in the hidden input layer?\n",
    "The top post in [this](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw) stackexchange question points to some good sources of information about this.\n",
    "\n",
    "> There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers'\n",
    "(Jeff Heaton, author of [Introduction to Neural Networks in Java](https://www.heatonresearch.com/book/) offers a few more.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 16,513\n",
      "Trainable params: 16,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Init the Sequential model\n",
    "model = Sequential()\n",
    "# Add Input layer\n",
    "model.add(InputLayer(input_shape=(images.shape[1],)))\n",
    "\n",
    "# Add hidden layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add output layer.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Finally, compile the model and print a summary. Loss function and optimizer is set during compilation.\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "The fit() function returns a history object that we'll store to plot how the training developed with epochs.\n",
    "First we need to set som training parameters.\n",
    "\n",
    "Note that if you would like to change something about the the model and run training again, you must\n",
    "re-compile the model. Otherwise you will essentially just be doubling the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for the training run. For the small set of 10k events you can probably get \n",
    "# away with even 100 epochs in a fairly small time, depending on your CPU.\n",
    "epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The validation_data keywork expects a 'tuple' (val_x, val_y) so we make one on the fly\n",
    "history = model.fit(\n",
    "    x=normalize_image_data(images[train_idx]),\n",
    "    y=labels[train_idx],\n",
    "    validation_data=(normalize_image_data(images[val_idx]), labels[val_idx]),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "Let's use the history object to plot how the loss and accuracy changed during training.\n",
    "The history contains a dictionary of the currently applied metrics as keys. In this case we have the\n",
    "keys \"loss\", \"accuracy\", \"val_loss\", and \"val_accuracy\". \"loss\" and \"accuracy\" pertain to the training data, while\n",
    "\"val_loss\" and \"val_accuracy\" are the metrics for the validation data we provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and val_loss as one plot, and accuracy and val_accuracy as another, side-by-side.\n",
    "# We'll also save this plot as plot_history() in the helper_functions file, so any time we need to\n",
    "# plot the history object like this we can just call that function and pass the history object as\n",
    "# an argument.\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "num_epochs = len(history.history['loss'])\n",
    "ax[0].plot(history.history['loss'], label='training')\n",
    "ax[0].plot(history.history['val_loss'], label='validation')\n",
    "ax[0].set_title(\"Model loss\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].set_xticks(np.arange(num_epochs))\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], label='training')\n",
    "ax[1].plot(history.history['val_accuracy'], label='validation')\n",
    "ax[1].set_title(\"Model accburacy\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy\")\n",
    "ax[1].set_xticks(np.arange(num_epochs))\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotting function will only plot these two metrics regardless of how many other metrics we add to the model,\n",
    "but it's a starting point that can be extended as you might add more metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "pred = model.predict([normalize_image_data(images[val_idx])])\n",
    "# Convert sigmoid values from prediction to integers so it works with the function.\n",
    "result = pred > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection of metrics\n",
    "Check out the notebook on logistic regression for details around the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(labels[val_idx], result)\n",
    "confmat = confusion_matrix(labels[val_idx], result)\n",
    "f1 = f1_score(labels[val_idx], result)\n",
    "mcc = matthews_corrcoef(labels[val_idx], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the metrics in an orderly fashion\n",
    "print(\"Confusion matrix:\\n\", confmat)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"MCC:\", mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-Curve and Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function we saved from the notebook on logistic regression\n",
    "plot_roc_auc(labels[val_idx], pred)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
